{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%schemas\n"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:748)\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2629)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2626)\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2716)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:95)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-e875c94de4db>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSparkSession\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mspark\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuilder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappName\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"test\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmaster\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"local[*]\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetOrCreate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/myanacondaenv/lib/python3.8/site-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36mgetOrCreate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    226\u001B[0m                             \u001B[0msparkConf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    227\u001B[0m                         \u001B[0;31m# This SparkContext may be an existing one.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 228\u001B[0;31m                         \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetOrCreate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msparkConf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    229\u001B[0m                     \u001B[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    230\u001B[0m                     \u001B[0;31m# by all sessions.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/myanacondaenv/lib/python3.8/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36mgetOrCreate\u001B[0;34m(cls, conf)\u001B[0m\n\u001B[1;32m    382\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    383\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 384\u001B[0;31m                 \u001B[0mSparkContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mSparkConf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    385\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    386\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/myanacondaenv/lib/python3.8/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[1;32m    144\u001B[0m         \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mgateway\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    145\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 146\u001B[0;31m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001B[0m\u001B[1;32m    147\u001B[0m                           conf, jsc, profiler_cls)\n\u001B[1;32m    148\u001B[0m         \u001B[0;32mexcept\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/myanacondaenv/lib/python3.8/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36m_do_init\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001B[0m\n\u001B[1;32m    207\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    208\u001B[0m         \u001B[0;31m# Create the Java SparkContext through Py4J\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 209\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mjsc\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_initialize_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_conf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    210\u001B[0m         \u001B[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    211\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_conf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkConf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_jconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/myanacondaenv/lib/python3.8/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36m_initialize_context\u001B[0;34m(self, jconf)\u001B[0m\n\u001B[1;32m    319\u001B[0m         \u001B[0mInitialize\u001B[0m \u001B[0mSparkContext\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mfunction\u001B[0m \u001B[0mto\u001B[0m \u001B[0mallow\u001B[0m \u001B[0msubclass\u001B[0m \u001B[0mspecific\u001B[0m \u001B[0minitialization\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    320\u001B[0m         \"\"\"\n\u001B[0;32m--> 321\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mJavaSparkContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    322\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    323\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mclassmethod\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/myanacondaenv/lib/python3.8/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1571\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1572\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_gateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1573\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1574\u001B[0m             answer, self._gateway_client, None, self._fqn)\n\u001B[1;32m   1575\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/myanacondaenv/lib/python3.8/site-packages/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:748)\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2629)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2626)\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2716)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:95)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"test\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               title|\n",
      "+--------------------+\n",
      "|Fantastic Beasts ...|\n",
      "|*Harry Potter and...|\n",
      "|*The Tales of Bee...|\n",
      "|*Harry Potter and...|\n",
      "|*Informix 12.10 o...|\n",
      "|Adventures of Huc...|\n",
      "|A Connecticut Yan...|\n",
      "|Jacques le Fataliste|\n",
      "|   A Woman in Berlin|\n",
      "|Spring Boot in Ac...|\n",
      "|Spring in Action:...|\n",
      "|Soft Skills: The ...|\n",
      "|*Java 8 in Action...|\n",
      "|              Hamlet|\n",
      "|             Pensées|\n",
      "|    *Fables choisies|\n",
      "|Discourse on Meth...|\n",
      "|       Twelfth Night|\n",
      "|             Macbeth|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}