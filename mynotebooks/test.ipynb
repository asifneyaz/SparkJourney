{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"wordcount\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "my_list = [('tony', 5), ('john', 2)]\n",
    "schema = ['name', 'age']\n",
    "rdd = sc.parallelize(my_list)\n",
    "rdd.collect()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('tony', 5), ('john', 2)]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%create rdd from list\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "dffromlist = spark.createDataFrame(rdd, schema)\n",
    "dffromlist.show()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|tony|  5|\n",
      "|john|  2|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%create dataframe from rdd\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from pyspark.sql.functions import col\n",
    "dffromlist.select(\"name\").show()\n",
    "dffromlist.withColumn(\"age10\", col(\"age\")*10).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|tony|\n",
      "|john|\n",
      "+----+\n",
      "\n",
      "+----+---+-----+\n",
      "|name|age|age10|\n",
      "+----+---+-----+\n",
      "|tony|  5|   50|\n",
      "|john|  2|   20|\n",
      "+----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%select column from dataframe and add a column\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "d = [{\"name\":\"Alice\", \"age\" : 12}, {\"name\":\"John\", \"age\" : 14}]\n",
    "df_d = spark.createDataFrame(d)\n",
    "df_d.show()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 12|Alice|\n",
      "| 14| John|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%create from dictionary\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "df_d2 = df_d.select(df_d.name, df_d.age)\n",
    "df_d2.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice| 12|\n",
      "| John| 14|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%reorder above dictionary to show name first and then age\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "from pyspark.sql import Row\n",
    "list_persons = [('matt',5),('john',2)]\n",
    "person_object = Row('name','age')\n",
    "rdd_list_person = sc.parallelize(list_persons)\n",
    "mapped_person = rdd_list_person.map(lambda rdd_item: person_object(*rdd_item))\n",
    "spark.createDataFrame(mapped_person).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|matt|  5|\n",
      "|john|  2|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%create from row object\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "wordsDF = spark.createDataFrame([('look',), ('spark',), ('tutorial',), ('spark',), ('look',), ('python',)], ['word'])\n",
    "wordsDF.show()\n",
    "print(type(wordsDF))\n",
    "wordsDF.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+\n",
      "|    word|\n",
      "+--------+\n",
      "|    look|\n",
      "|   spark|\n",
      "|tutorial|\n",
      "|   spark|\n",
      "|    look|\n",
      "|  python|\n",
      "+--------+\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- word: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "from pyspark.sql.functions import length\n",
    "wordsLengthsDF = wordsDF.select(wordsDF.word, length('word').alias('lengths'))  # transformation\n",
    "wordsLengthsDF.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+-------+\n",
      "|    word|lengths|\n",
      "+--------+-------+\n",
      "|    look|      4|\n",
      "|   spark|      5|\n",
      "|tutorial|      8|\n",
      "|   spark|      5|\n",
      "|    look|      4|\n",
      "|  python|      6|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%show lengths of each word\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "wordCountsDF = wordsDF.groupBy('word').count()\n",
    "wordCountsDF.show()\n",
    "wordCountsDF.orderBy(col(\"count\").desc()).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+-----+\n",
      "|    word|count|\n",
      "+--------+-----+\n",
      "|tutorial|    1|\n",
      "|   spark|    2|\n",
      "|    look|    2|\n",
      "|  python|    1|\n",
      "+--------+-----+\n",
      "\n",
      "+--------+-----+\n",
      "|    word|count|\n",
      "+--------+-----+\n",
      "|   spark|    2|\n",
      "|    look|    2|\n",
      "|  python|    1|\n",
      "|tutorial|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%no of times a word appears\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "words2df = spark.createDataFrame([(\"spark\",),(\"spark\",),(\"hello\",),(\"hello\",),(\"hi\",)],[\"name\"])\n",
    "countdf = words2df.groupby(words2df.name).count()\n",
    "countdf.orderBy(col(\"count\").asc()).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+-----+\n",
      "| name|count|\n",
      "+-----+-----+\n",
      "|   hi|    1|\n",
      "|spark|    2|\n",
      "|hello|    2|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "from pyspark.sql.functions import split,explode\n",
    "fileName = \"/Users/tech/codes/SparkJourney/data/PrideandPrejudice.txt\"\n",
    "bookDF = spark.read.text(fileName).select(col(\"value\").alias(\"lines\"))\n",
    "bookWordsSplitDF = bookDF.select(split(bookDF.lines, ' ').alias(\"split\"))\n",
    "bookWordsSingleDF = bookWordsSplitDF.select(explode(bookWordsSplitDF.split).alias(\"word\"))\n",
    "bookWordCountDF = bookWordsSingleDF.groupby(bookWordsSingleDF.word).count().orderBy(col(\"count\").desc())\n",
    "bookWordCountDF.show()\n",
    "\n",
    "# bookWordsSplitDF = bookDF.select(split(bookDF.lines, ' ').alias('split'))\n",
    "# bookWordsSingleDF = bookWordsSplitDF.select(explode(bookWordsSplitDF.split).alias(\"word\"))\n",
    "# bookWordsSingleDF.groupby(bookWordsSingleDF.word).count().orderBy(col(\"count\").desc()).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "|    |72884|\n",
      "| the| 4218|\n",
      "|  to| 4123|\n",
      "|  of| 3666|\n",
      "| and| 3314|\n",
      "|   a| 1944|\n",
      "| her| 1855|\n",
      "|  in| 1816|\n",
      "| was| 1798|\n",
      "|   I| 1724|\n",
      "|that| 1417|\n",
      "| not| 1365|\n",
      "| she| 1304|\n",
      "|  be| 1206|\n",
      "| his| 1167|\n",
      "| had| 1126|\n",
      "|  as| 1121|\n",
      "|with| 1040|\n",
      "|  he| 1039|\n",
      "| for| 1004|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('myanacondaenv': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "f047f6ce52878136e5204442ea76ae6724ca4e0c389074ef753143ef98594919"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}